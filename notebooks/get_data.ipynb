{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e21b666-c743-4465-a47a-b8880f1d634c",
   "metadata": {},
   "source": [
    "## Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b6858d-1b7c-4dfe-b3ca-35d34a10956b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gspread\n",
    "from pathlib import Path\n",
    "\n",
    "# Path(__file__) gives the full path to this file.\n",
    "# .resolve() makes it an absolute path.\n",
    "# .parent gives the directory containing this file (i.e., 'src/').\n",
    "# .parent.parent gives the parent of 'src/' (i.e., your_project_directory/).\n",
    "\n",
    "def get_authenticated_gspread_client() -> gspread.Client | None:\n",
    "    try:\n",
    "        gc = gspread.oauth(\n",
    "            credentials_filename=\"credentials.json\",\n",
    "            authorized_user_filename=\"token.json\",\n",
    "        )\n",
    "        return gc\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a1816-71e4-47f6-94f0-e9fa7883cba1",
   "metadata": {},
   "source": [
    "## Set up constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bf2c46-174d-4112-9855-c1a9e5f60a3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "import logging\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "DEFAULT_HEADER_ROW = 12\n",
    "DEFAULT_DATA_START_ROW = 14\n",
    "MAX_COLUMN_LETTER = \"U\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "COLNAME_REPLACEMENTS = {\" \": \"_\", \"/\": \"\", \"#\": \"no\", \"__\": \"_\"}\n",
    "\n",
    "# Define standard value replacements (Resolve ambiguity of 'X' and '')\n",
    "# Suggestion: Use more descriptive keys if possible\n",
    "VALUE_REPLACEMENTS = {\n",
    "    # For 'is_complete' column primarily\n",
    "    \"Not Started\": False,\n",
    "    \"Done\": True,\n",
    "    \"In Process\": False,\n",
    "    \"Hold/ Change in Contract\": True,\n",
    "    # For 'projected_hours'\n",
    "    \"X\": 1.25,  # TODO: Confirm meaning of 'X'\n",
    "    \"\": 1.25,  # TODO: Confirm meaning of blank \"\"\n",
    "}\n",
    "\n",
    "# Define columns needed for the final outputs\n",
    "DATA_COLS = [\n",
    "    \"site_no\",\n",
    "    \"address\",\n",
    "    \"work_description\",\n",
    "    \"owner_phone_comments\",\n",
    "    \"no_parks\",\n",
    "    \"nbw\",\n",
    "    \"projected_hours\",\n",
    "    \"flagging\",\n",
    "    \"requires_squirt_boom\",\n",
    "    \"merge\",\n",
    "    \"notes\",\n",
    "    \"also_clear_for\",\n",
    "    \"is_complete\",\n",
    "]\n",
    "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1M__pvslmhMRkXCl-7DEPc0PzvKj6qlgfc8antAd9hgI/edit#gid=223128104\"\n",
    "CIRCUIT_NAME = \"2780\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb867a6-d733-49d9-a562-fe55d8d74fdd",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff278a6e-cd76-4747-b0a6-375fb990dfc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SpreadsheetCleaner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: gspread.Client,\n",
    "        sheet_url: str = None,\n",
    "        circuit_name: str = None,\n",
    "        header_row: int = DEFAULT_HEADER_ROW,\n",
    "        data_start_row: int = DEFAULT_DATA_START_ROW,\n",
    "        max_column_letter: str = MAX_COLUMN_LETTER,\n",
    "        colname_replacements: dict = None,\n",
    "        value_replacements: dict = None,\n",
    "        data_cols: list = None\n",
    "        \n",
    "    ):\n",
    "        self.client = client\n",
    "        self.sheet_url = sheet_url if sheet_url is not None else SHEET_URL # TODO remove for prod\n",
    "        self.circuit_name = circuit_name if circuit_name is not None else CIRCUIT_NAME # TODO remove for prod\n",
    "        self.header_row = header_row\n",
    "        self.max_column_letter = max_column_letter\n",
    "        self.data_start_row = data_start_row\n",
    "        self.colname_replacements = colname_replacements if colname_replacements is not None else COLNAME_REPLACEMENTS.copy()\n",
    "        self.value_replacements = self.value_replacements = value_replacements if value_replacements is not None else VALUE_REPLACEMENTS.copy()\n",
    "        self.data_cols = data_cols if data_cols is not None else DATA_COLS.copy()\n",
    "        self.df: pd.DataFrame | None = None\n",
    "        self.raw_column_names: list | None = None\n",
    "        \n",
    "        logger.info(f\"SpreadsheetCleaner initialized for worksheet: '{self.circuit_name}'.\")\n",
    "\n",
    "    def _fetch_raw_df(self):\n",
    "        logger.info(\n",
    "            f\"clean_spreadsheet_data called for circuit: '{self.circuit_name}' to return a single DataFrame.\"\n",
    "        )\n",
    "        if self.header_row < 1 or self.data_start_row <= self.header_row:\n",
    "            logger.error(\"Invalid header_row_num or data_start_row_num.\")\n",
    "            raise ValueError(\n",
    "                \"Header row must be less than data start row, and both positive.\"\n",
    "            )\n",
    "    \n",
    "        try:\n",
    "            spreadsheet = self.client.open_by_url(self.sheet_url)\n",
    "            worksheet = spreadsheet.worksheet(self.circuit_name)\n",
    "            logger.info(f\"Successfully opened worksheet: '{self.circuit_name}'\")\n",
    "    \n",
    "            all_sheet_values = worksheet.get(f\"A{self.header_row}:{self.max_column_letter}\")\n",
    "            if not all_sheet_values:\n",
    "                logger.warning(f\"No data found in '{self.circuit_name}' from row {self.header_row}.\")\n",
    "                self.df = pd.DataFrame(\n",
    "                    columns=self.data_cols\n",
    "                )\n",
    "                return True\n",
    "    \n",
    "            raw_column_names = all_sheet_values[0]\n",
    "            data_rows_start_index_in_fetched_list = self.data_start_row - self.header_row\n",
    "            data_values = all_sheet_values[data_rows_start_index_in_fetched_list:]\n",
    "    \n",
    "            if not data_values:\n",
    "                logger.warning(f\"No data rows found after header in '{self.circuit_name}'.\")\n",
    "                self.df = pd.DataFrame(columns=self.data_cols)\n",
    "                return True\n",
    "                \n",
    "            # --- Pad data_values if rows are shorter than raw_column_names ---\n",
    "            num_expected_cols = len(raw_column_names)\n",
    "            data_values_padded = []\n",
    "            for i, row in enumerate(data_values):\n",
    "                row_len = len(row)\n",
    "                if row_len < num_expected_cols:\n",
    "                    # Pad the row with empty strings (or None, or any placeholder)\n",
    "                    padding = [\"\"] * (num_expected_cols - row_len) \n",
    "                    padded_row = row + padding\n",
    "                    data_values_padded.append(padded_row)\n",
    "                    if i == 0: # Log for the first problematic row for easier debugging\n",
    "                         logger.debug(f\"Padded first data row. Original length: {row_len}, Target length: {num_expected_cols}. Original row: {row[:10]}... Padded row: {padded_row[:10]}...\")\n",
    "                else:\n",
    "                    # If row is already long enough (or longer, though pandas will truncate if more cols than names)\n",
    "                    data_values_padded.append(row[:num_expected_cols]) # Ensure it's not longer\n",
    "\n",
    "                data_values = data_values_padded # Use the padded data\n",
    "\n",
    "            if len(raw_column_names) < len(self.data_cols):\n",
    "                logger.error(\n",
    "                    f\"CRITICAL COLUMN COUNT MISMATCH for worksheet '{self.circuit_name}': \"\n",
    "                    f\"Cannot create DataFrame. Header columns: {len(raw_column_names)}, Data columns: {len(data_values[0])}. \"\n",
    "                    f\"Check the header row and data start row.\"\n",
    "                    f\"This may indicate a structural change in the sheet.\"\n",
    "                    f\"If the head row is correct, check DATA_COLS and adjust if needed.\"\n",
    "                )\n",
    "                return False\n",
    "    \n",
    "            self.df = pd.DataFrame(data_values, columns=raw_column_names)\n",
    "            logger.info(\n",
    "                f\"Initial DataFrame: {self.df.shape[0]} rows, {self.df.shape[1]} columns.\"\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except gspread.exceptions.SpreadsheetNotFound:\n",
    "            logger.error(f\"Spreadsheet not found: {sheet_url}\")\n",
    "            return False\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            logger.error(f\"Worksheet '{self.circuit_name}' not found in {sheet_url}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing '{self.circuit_name}': {e}\", exc_info=True)\n",
    "            return False\n",
    "\n",
    "    def _normalize_column_names(self):\n",
    "        cleaned_cols = self.df.columns.str.strip().str.lower()\n",
    "        for char, replacement in self.colname_replacements.items():\n",
    "            cleaned_cols = cleaned_cols.str.replace(char, replacement, regex=False)\n",
    "\n",
    "        self.df.columns = cleaned_cols\n",
    "        rename_map = {\"squirt_boom\": \"requires_squirt_boom\", \"status\": \"is_complete\"}\n",
    "        \n",
    "        # --- Dynamically add to rename_map for pattern-based 'site_no' ---\n",
    "        site_id_found_raw_name = None # Initialize to track if we've found a site_id column\n",
    "        \n",
    "        for col_name in self.df.columns: # Iterate through the *now cleaned* column names\n",
    "            if col_name.startswith(\"site_no\") or col_name.startswith(\"site_num\"):\n",
    "                if site_id_found_raw_name: # Already found one\n",
    "                    logger.warning(\n",
    "                        f\"Multiple columns found starting with 'site_no/site_num': \"\n",
    "                        f\"'{site_id_found_raw_name}' and '{col_name}'. \"\n",
    "                        f\"Using the first one found: '{site_id_found_raw_name}'.\"\n",
    "                    )\n",
    "                else: # This is the first one we've found\n",
    "                    site_id_found_raw_name = col_name # Store the name we found\n",
    "                    logger.debug(f\"Identified column '{site_id_found_raw_name}' to be renamed to 'site_no'.\")\n",
    "                    rename_map[site_id_found_raw_name] = \"site_no\" # Add to map: current_name -> new_standard_name\n",
    "                                                                          # Changed \"site_no\" to \"site_id_standard\" for clarity\n",
    "\n",
    "        if not site_id_found_raw_name:\n",
    "            logger.warning(\"No column found starting with 'site_no' or 'site_num'. Standard 'site_no' column will not be created through renaming.\")\n",
    "        \n",
    "        # Perform the rename operation using the accumulated map\n",
    "        if rename_map: # Only rename if there's something in the map\n",
    "            self.df.rename(columns=rename_map, inplace=True) # Use inplace=True to modify df directly\n",
    "            logger.debug(\n",
    "                f\"Columns after renaming: {self.df.columns.tolist()}\"\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            logger.debug(\"No columns were identified for renaming based on the rename_map\")\n",
    "\n",
    "    def _get_data_rows(self):\n",
    "        \"\"\"\n",
    "        Filters self.df to include rows from the first row where 'site_no' is numeric\n",
    "        to the last row where 'site_no' is numeric. Modifies self.df in place.\n",
    "        \"\"\"\n",
    "        if self.df is None or self.df.empty:\n",
    "            logger.warning(\"DataFrame is empty or None. Cannot filter by 'site_no' range.\")\n",
    "            return\n",
    "\n",
    "        key_column = 'site_no' # The column to check for numeric values\n",
    "\n",
    "        if key_column not in self.df.columns:\n",
    "            logger.error(f\"Key column '{key_column}' not found in DataFrame. Cannot filter by range.\")\n",
    "            # Optionally, you might want to set self.df to an empty frame or handle this error\n",
    "            return\n",
    "\n",
    "        # --- Find the first potential data row ---\n",
    "        # We're looking for something that can be an integer.\n",
    "        # `errors='coerce'` will turn non-numbers into NaN.\n",
    "        numeric_series = pd.to_numeric(self.df[key_column], errors='coerce')\n",
    "        \n",
    "        # A valid site number for starting the block must be a number (not NaN)\n",
    "        potential_start_indices = self.df[numeric_series.notna()].index\n",
    "    \n",
    "        if potential_start_indices.empty:\n",
    "            logger.warning(f\"No numeric-like values found in '{key_column}' to start data block.\")\n",
    "            self.df = pd.DataFrame(columns=self.df.columns) # Make DF empty\n",
    "            return\n",
    "    \n",
    "        first_potential_data_index_label = potential_start_indices.min()\n",
    "        \n",
    "        # --- Find the end of the contiguous data block ---\n",
    "        last_contiguous_data_index_label = first_potential_data_index_label\n",
    "        num_of_contiguous_invalid = 0 #only cut if multiple (3) NaN in a row to account for regular missing data\n",
    "        \n",
    "        # Get the integer position of the first potential data row\n",
    "        try:\n",
    "            start_pos = self.df.index.get_loc(first_potential_data_index_label)\n",
    "        except KeyError:\n",
    "            logger.error(f\"Could not find index label {first_potential_data_index_label} after filtering. This should not happen.\")\n",
    "            return\n",
    "    \n",
    "        for i in range(start_pos, len(self.df)):\n",
    "            current_index_label = self.df.index[i]\n",
    "            current_value_in_key_col = self.df.loc[current_index_label, key_column]\n",
    "            # Define what a \"valid ongoing data row\" looks like\n",
    "            # For site_no, it's not NaN (for now)\n",
    "            # If it's blank or text like \"Total Sites\", the block has ended.\n",
    "            is_valid_ongoing_site_no = False\n",
    "            if pd.notna(current_value_in_key_col) and str(current_value_in_key_col).strip() != \"\":\n",
    "                is_valid_ongoing_site_no = True \n",
    "                    # Add more specific checks if needed, e.g. positive, within a range.\n",
    "            \n",
    "            if is_valid_ongoing_site_no:\n",
    "                last_contiguous_data_index_label = current_index_label\n",
    "                num_of_contiguous_invalid = 0\n",
    "                \n",
    "            elif not is_valid_ongoing_site_no and num_of_contiguous_invalid < 3:\n",
    "                num_of_contiguous_invalid += 1\n",
    "                logger.debug(f\"Invalid row encountered at index {current_index_label} (value: '{self.df.loc[current_index_label, key_column]}'). \"\n",
    "                 f\"Contiguous invalid count: {num_of_contiguous_invalid}\")\n",
    "                \n",
    "            else:\n",
    "                last_contiguous_data_index_label = last_contiguous_data_index_label\n",
    "                # This row is no longer a valid site number, so the block ended at the previous row.\n",
    "                logger.debug(f\"End of contiguous '{key_column}' block detected at index before {current_index_label} \"\n",
    "                             f\"(value: '{current_value_in_key_col}')\")\n",
    "                break # Stop iterating\n",
    "    \n",
    "        logger.info(f\"Dynamically identified data block for '{key_column}': \"\n",
    "                    f\"starts at index label {first_potential_data_index_label}, \"\n",
    "                    f\"ends at index label {last_contiguous_data_index_label}.\")\n",
    "    \n",
    "        self.df = self.df.loc[first_potential_data_index_label:last_contiguous_data_index_label].copy()\n",
    "        logger.info(f\"DataFrame sliced to contiguous '{key_column}' block. New shape: {self.df.shape}\")\n",
    "        \n",
    "            \n",
    "    def _apply_value_transformations(self):\n",
    "        self.df.replace(self.value_replacements, inplace=True)\n",
    "        self.df = self.df.infer_objects(copy=False)\n",
    "        logger.info(\"Applied general value_replacements and inferred objects.\")\n",
    "\n",
    "\n",
    "        if \"requires_squirt_boom\" in self.df.columns:\n",
    "            true_values = [\"TRUE\", \"T\", \"YES\", \"Y\", \"1\"]\n",
    "            self.df[\"requires_squirt_boom\"] = (\n",
    "                self.df[\"requires_squirt_boom\"]\n",
    "                .astype(str)\n",
    "                .str.upper()\n",
    "                .isin(true_values)\n",
    "            )\n",
    "            logger.debug(\"Converted 'requires_squirt_boom' to boolean.\")\n",
    "\n",
    "        if \"is_complete\" in self.df.columns:\n",
    "            # Ensure 'is_complete' is boolean after VALUE_REPLACEMENTS\n",
    "            # Handles cases where a value wasn't in the map and might be e.g. a number if \"X\" was in that column\n",
    "            self.df[\"is_complete\"] = self.df[\"is_complete\"].apply(\n",
    "                lambda x: x if isinstance(x, bool) else False\n",
    "            )\n",
    "            logger.debug(\"Ensured 'is_complete' is boolean.\")\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"'is_complete' column not found after cleaning. Cannot filter by completion status.\"\n",
    "            )\n",
    "    def _filter_and_select_final_columns(self):\n",
    "        \n",
    "        available_final_cols = [col for col in self.data_cols if col in self.df.columns]\n",
    "        missing_final_cols = [\n",
    "            col for col in self.data_cols if col not in available_final_cols\n",
    "        ]\n",
    "        if missing_final_cols:\n",
    "            logger.warning(\n",
    "                f\"For the final DataFrame, missing expected columns: {missing_final_cols}. They will be excluded.\"\n",
    "            )\n",
    "\n",
    "        # Filter for incomplete items\n",
    "        \"\"\"\n",
    "        if \"is_complete\" in self.df.columns and not self.df.empty:\n",
    "            # Ensure the column exists before trying to filter on it\n",
    "            self.df = self.df.loc[\n",
    "                self.df[\"is_complete\"] == False, available_final_cols\n",
    "            ].copy()\n",
    "        elif not self.df.empty:\n",
    "            # 'is_complete' column is missing, return available columns without this filter\n",
    "            logger.warning(\n",
    "                \"Cannot filter by 'is_complete' as column is missing. Returning selected columns unfiltered by completion.\"\n",
    "            )\n",
    "            self.df = self.df.loc[:, available_final_cols].copy()\n",
    "        else:  # self.df is empty\n",
    "            self.df = pd.DataFrame(columns=available_final_cols)\n",
    "        logger.info(f\"Filtering complete. Final DataFrame shape: {self.df.shape}\")\n",
    "\"\"\"\n",
    "    def clean(self):\n",
    "        logger.info(f\"Starting cleaning process for worksheet: {self.circuit_name}...\")\n",
    "        if not self._fetch_raw_df():\n",
    "            logger.error(\"Failed to fetch initial data. Aborting cleaning process.\")\n",
    "            # _fetch_data might return an empty df with specific columns if no data was found\n",
    "            # but still indicates a \"success\" in terms of not having an exception.\n",
    "            # If _fetch_data returns False due to an exception or critical issue:\n",
    "            if self.df is None: # If df was never even initialized due to critical fetch error\n",
    "                 return None\n",
    "            # The current _fetch_data returns False on critical error, or if no data (sets self.df to empty)\n",
    "            # if _fetch_data returns False, we assume a critical issue.\n",
    "            # If self.df is already an empty DataFrame with final_output_cols, that's fine.\n",
    "            if self.df is not None and self.df.empty and list(self.df.columns) == self.final_output_cols:\n",
    "                 logger.warning(\"No data to process, returning empty DataFrame with expected columns.\")\n",
    "                 return self.df\n",
    "            return None # Critical fetch error\n",
    "\n",
    "        self._normalize_column_names()\n",
    "        self._get_data_rows()\n",
    "        self._apply_value_transformations()\n",
    "        self._filter_and_select_final_columns() # Corrected method name\n",
    "        \n",
    "        logger.info(f\"Cleaning process finished. Final DataFrame shape: {self.df.shape if self.df is not None else 'None'}\")\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a718c5df-6296-40df-8e5a-c9b199e6fd47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1        1\n",
       "2        2\n",
       "3        3\n",
       "4        4\n",
       "5        5\n",
       "      ... \n",
       "699    699\n",
       "700    700\n",
       "701    701\n",
       "702    702\n",
       "703    703\n",
       "Name: site_no, Length: 703, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_client = get_authenticated_gspread_client()\n",
    "\n",
    "cleaner = SpreadsheetCleaner(\n",
    "    client=g_client,\n",
    "    circuit_name=CIRCUIT_NAME,\n",
    "    sheet_url=SHEET_URL,\n",
    ")\n",
    "data_df = cleaner.clean()\n",
    "print(data_df.shape)\n",
    "data_df[\"site_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f707f3-3d9b-4b00-81f4-932afef078e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"full_address\"].to_csv('addresses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kempv2)",
   "language": "python",
   "name": ".kempv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
